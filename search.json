[
  {
    "objectID": "pages/slt/logistic-regression/index.html",
    "href": "pages/slt/logistic-regression/index.html",
    "title": "üéì Logistic Regression Essentials",
    "section": "",
    "text": "Ajvad Haneef, Senior Research Fellow, Department of Computer Science & Engineering, NIT Calicut \n\n\n\n\n\nLogistic Regression\n\n\n\n\nLogistic Regression is one of the most fundamental and widely used algorithms for classification tasks in Machine Learning.\n\nLinear Regression predicts a continuous value (like house price, temperature).\nLogistic Regression predicts a category or probability (like: Will this customer click an ad? Yes/No.¬†Is this email spam? Yes/No).\n\nThe key idea is that instead of predicting the value \\(Y\\) itself, Logistic Regression predicts the probability \\(P(Y=1)\\) that a given input belongs to a certain class. It then uses a special function, called the Sigmoid Function to squash any output into a probability score between 0 and 1. The Sigmoid Function is defined as:\n\\(P(Y=1) = \\frac{1}{1 + e^{-(X \\cdot W)}}\\) ,\nThe Sigmoid function takes the linear combination of input features (\\(X\\)) and weights (\\(W\\)) and transforms it into a probability.\nThe output can then be thresholded (commonly at 0.5) to make a binary classification decision:\n\nIf \\(P(Y=1) \\geq 0.5\\), predict class 1 (YES).\nIf \\(P(Y=1) &lt; 0.5\\), predict class 0 (NO).\n\nThe Sigmoid function has an ‚ÄúS‚Äù-shaped curve that smoothly transitions from 0 to 1, making it ideal for modeling probabilities.\nThe logistic curve is shown below: \n\n\nImagine a ‚ÄúProbability Gatekeeper‚Äù with a dial that can only point to values between 0 and 1.\n\nA standard Linear Regression model first calculates a raw score for a data point (this score can be any number, e.g., \\(-10\\), \\(0\\), \\(50\\)). This is the input to the Gatekeeper.\nThe Gatekeeper (the Sigmoid Function) takes this raw score and transforms it:\n\nVery large positive scores get turned into a probability close to \\(\\mathbf{1}\\) (High Confidence: YES).\nVery large negative scores get turned into a probability close to \\(\\mathbf{0}\\) (High Confidence: NO).\nA score of \\(\\mathbf{0}\\) gets turned into a probability of \\(\\mathbf{0.5}\\) (Uncertain).\n\n\nThis transformation allows us to use a linear model for a non-linear classification problem.\n\n\n\n\n\n\n\nCore Idea: Linear Algebra provides the blueprint for how we structure data and calculate the raw score.\n\n\n\n\n\n\n\n\nComponent\nLinear Algebra Mapping\nAnalogy\n\n\n\n\nInput Features\nA Design Matrix (\\(X\\))\nA collection of ingredients for a recipe (e.g., age, income, time spent on site).\n\n\nModel Parameters\nA Weight Vector (\\(W\\))\nA scaling factor for each ingredient (how important is age vs.¬†income?).\n\n\nRaw Score Calculation\nMatrix-Vector Multiplication (\\(X \\cdot W\\))\nThe initial mixing of ingredients to get a base flavor (the raw score).\n\n\n\nJust as in linear regression, the initial step in logistic regression is fundamentally a linear transformation: a weighted sum of the input features. This provides the logit, or log-odds, which is the raw, unbounded score that the Sigmoid function will process.\n\n\n\nCore Idea: Probability & Statistics provides the specific function to model the probability and the metric to measure the model‚Äôs performance.\n\n\n\n\n\n\n\n\nComponent\nStatistical Mapping\nAnalogy\n\n\n\n\nPrediction Function\nSigmoid Function (\\(P = \\frac{1}{1 + e^{-(X \\cdot W)}}\\))\nThe Probability Gatekeeper that converts the raw score into a \\(0\\) to \\(1\\) probability.\n\n\nLoss Function\nLog-Loss (or Binary Cross-Entropy)\nThe Critic‚Äôs Scorecard. Instead of measuring squared distance, it heavily penalizes confident wrong predictions.\n\n\n\nIn classification, we care about assigning the correct probability to the correct class. If the model predicts an event will happen with \\(P=0.9\\) (90% sure) but it doesn‚Äôt happen, the Log-Loss gives a massive penalty. Conversely, if it predicts \\(P=0.1\\) and the event doesn‚Äôt happen, the penalty is small. This function is derived from the principle of Maximum Likelihood Estimation, which seeks the model parameters that make the observed data most probable.\n\\[L(W) = - \\frac{1}{N} \\sum_{i=1}^N \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\\]\n\n\n\nCore Idea: Calculus provides the tool to efficiently determine how to adjust the model parameters to minimize the error.\n\n\n\n\n\n\n\n\nComponent\nCalculus Mapping\nAnalogy\n\n\n\n\nOptimization Direction\nThe Gradient (\\(\\nabla L\\))\nThe Compass Needle. It points in the direction of steepest ascent (worst performance).\n\n\nLearning Mechanism\nPartial Derivatives\nThe Sensitivity Report. For every weight, it tells you: ‚ÄúIf I slightly increase this weight, how much will the total error change?‚Äù\n\n\n\nTo make the model better, we must take steps in the opposite direction of the gradient (\\(\\mathbf{- \\nabla L}\\)). Calculating the gradient of the complex Log-Loss function involves the Chain Rule, where we compute the derivative of the loss with respect to the prediction, then the derivative of the prediction (Sigmoid) with respect to the raw score, and finally the derivative of the raw score with respect to the weights. This systematic calculation is the engine of learning.\n\n\n\nCore Idea: Optimization is the strategy for navigating the error landscape and finding the best set of weights.\n\n\n\n\n\n\n\n\nComponent\nOptimization Mapping\nAnalogy\n\n\n\n\nAlgorithm\nGradient Descent (or its variants)\nThe Mountain Climber. The climber starts somewhere on the error mountain and takes repeated, calculated steps downhill.\n\n\nLearning Rate\nStep Size (\\(\\alpha\\))\nThe Length of the Stride. Too large, and you might jump right over the minimum; too small, and the journey will take forever.\n\n\nStopping Condition\nConvergence Criteria\nThe Summit Signal. Stop climbing when your steps downhill no longer meaningfully improve your altitude (error).\n\n\n\nOptimization takes the direction provided by Calculus and applies a disciplined strategy (Gradient Descent) to find the weights that minimize the Log-Loss. By iteratively updating the weight vector \\(W\\) using the rule: \\(W_{new} = W_{old} - \\alpha \\cdot \\nabla L\\), we ensure the model continuously improves its ability to correctly separate the classes.\n\n\n\n\n\nHere is a simple example using the popular scikit-learn library to perform a Logistic Regression classification.\n# Import necessary libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n# 1. Prepare Synthetic Data (Feature X, Target Y)\n# X: Study hours, Y: Pass/Fail (1/0)\nX = np.array([[0.5], [1.0], [2.2], [3.1], [4.5], [5.0], [6.8], [7.2], [8.0], [9.5]])\ny = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1]) # 1 = Pass, 0 = Fail\n\n# Reshape X for scikit-learn\nX = X.reshape(-1, 1)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# 2. Initialize and Train the Model\nmodel = LogisticRegression()\n# The .fit() method here is the Optimization process (Gradient Descent)\nmodel.fit(X_train, y_train) \n\n# 3. Make Predictions\n# Predict class labels (0 or 1)\npredictions = model.predict(X_test)\n# Predict probabilities (e.g., 0.95, 0.12)\nprobabilities = model.predict_proba(X_test) \n\n# 4. Evaluate Performance\naccuracy = accuracy_score(y_test, predictions)\n\nprint(f\"Model Intercept (b0): {model.intercept_[0]:.2f}\")\nprint(f\"Model Coefficient (w1): {model.coef_[0][0]:.2f}\")\nprint(f\"Accuracy on Test Data: {accuracy:.2f}\")\nprint(\"\\nProbabilities for Test Data:\")\nprint(probabilities)"
  },
  {
    "objectID": "pages/slt/logistic-regression/index.html#what-is-logistic-regression",
    "href": "pages/slt/logistic-regression/index.html#what-is-logistic-regression",
    "title": "üéì Logistic Regression Essentials",
    "section": "",
    "text": "Logistic Regression is one of the most fundamental and widely used algorithms for classification tasks in Machine Learning.\n\nLinear Regression predicts a continuous value (like house price, temperature).\nLogistic Regression predicts a category or probability (like: Will this customer click an ad? Yes/No.¬†Is this email spam? Yes/No).\n\nThe key idea is that instead of predicting the value \\(Y\\) itself, Logistic Regression predicts the probability \\(P(Y=1)\\) that a given input belongs to a certain class. It then uses a special function, called the Sigmoid Function to squash any output into a probability score between 0 and 1. The Sigmoid Function is defined as:\n\\(P(Y=1) = \\frac{1}{1 + e^{-(X \\cdot W)}}\\) ,\nThe Sigmoid function takes the linear combination of input features (\\(X\\)) and weights (\\(W\\)) and transforms it into a probability.\nThe output can then be thresholded (commonly at 0.5) to make a binary classification decision:\n\nIf \\(P(Y=1) \\geq 0.5\\), predict class 1 (YES).\nIf \\(P(Y=1) &lt; 0.5\\), predict class 0 (NO).\n\nThe Sigmoid function has an ‚ÄúS‚Äù-shaped curve that smoothly transitions from 0 to 1, making it ideal for modeling probabilities.\nThe logistic curve is shown below: \n\n\nImagine a ‚ÄúProbability Gatekeeper‚Äù with a dial that can only point to values between 0 and 1.\n\nA standard Linear Regression model first calculates a raw score for a data point (this score can be any number, e.g., \\(-10\\), \\(0\\), \\(50\\)). This is the input to the Gatekeeper.\nThe Gatekeeper (the Sigmoid Function) takes this raw score and transforms it:\n\nVery large positive scores get turned into a probability close to \\(\\mathbf{1}\\) (High Confidence: YES).\nVery large negative scores get turned into a probability close to \\(\\mathbf{0}\\) (High Confidence: NO).\nA score of \\(\\mathbf{0}\\) gets turned into a probability of \\(\\mathbf{0.5}\\) (Uncertain).\n\n\nThis transformation allows us to use a linear model for a non-linear classification problem."
  },
  {
    "objectID": "pages/slt/logistic-regression/index.html#deconstructing-logistic-regression-through-the-four-lenses",
    "href": "pages/slt/logistic-regression/index.html#deconstructing-logistic-regression-through-the-four-lenses",
    "title": "üéì Logistic Regression Essentials",
    "section": "",
    "text": "Core Idea: Linear Algebra provides the blueprint for how we structure data and calculate the raw score.\n\n\n\n\n\n\n\n\nComponent\nLinear Algebra Mapping\nAnalogy\n\n\n\n\nInput Features\nA Design Matrix (\\(X\\))\nA collection of ingredients for a recipe (e.g., age, income, time spent on site).\n\n\nModel Parameters\nA Weight Vector (\\(W\\))\nA scaling factor for each ingredient (how important is age vs.¬†income?).\n\n\nRaw Score Calculation\nMatrix-Vector Multiplication (\\(X \\cdot W\\))\nThe initial mixing of ingredients to get a base flavor (the raw score).\n\n\n\nJust as in linear regression, the initial step in logistic regression is fundamentally a linear transformation: a weighted sum of the input features. This provides the logit, or log-odds, which is the raw, unbounded score that the Sigmoid function will process.\n\n\n\nCore Idea: Probability & Statistics provides the specific function to model the probability and the metric to measure the model‚Äôs performance.\n\n\n\n\n\n\n\n\nComponent\nStatistical Mapping\nAnalogy\n\n\n\n\nPrediction Function\nSigmoid Function (\\(P = \\frac{1}{1 + e^{-(X \\cdot W)}}\\))\nThe Probability Gatekeeper that converts the raw score into a \\(0\\) to \\(1\\) probability.\n\n\nLoss Function\nLog-Loss (or Binary Cross-Entropy)\nThe Critic‚Äôs Scorecard. Instead of measuring squared distance, it heavily penalizes confident wrong predictions.\n\n\n\nIn classification, we care about assigning the correct probability to the correct class. If the model predicts an event will happen with \\(P=0.9\\) (90% sure) but it doesn‚Äôt happen, the Log-Loss gives a massive penalty. Conversely, if it predicts \\(P=0.1\\) and the event doesn‚Äôt happen, the penalty is small. This function is derived from the principle of Maximum Likelihood Estimation, which seeks the model parameters that make the observed data most probable.\n\\[L(W) = - \\frac{1}{N} \\sum_{i=1}^N \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\\]\n\n\n\nCore Idea: Calculus provides the tool to efficiently determine how to adjust the model parameters to minimize the error.\n\n\n\n\n\n\n\n\nComponent\nCalculus Mapping\nAnalogy\n\n\n\n\nOptimization Direction\nThe Gradient (\\(\\nabla L\\))\nThe Compass Needle. It points in the direction of steepest ascent (worst performance).\n\n\nLearning Mechanism\nPartial Derivatives\nThe Sensitivity Report. For every weight, it tells you: ‚ÄúIf I slightly increase this weight, how much will the total error change?‚Äù\n\n\n\nTo make the model better, we must take steps in the opposite direction of the gradient (\\(\\mathbf{- \\nabla L}\\)). Calculating the gradient of the complex Log-Loss function involves the Chain Rule, where we compute the derivative of the loss with respect to the prediction, then the derivative of the prediction (Sigmoid) with respect to the raw score, and finally the derivative of the raw score with respect to the weights. This systematic calculation is the engine of learning.\n\n\n\nCore Idea: Optimization is the strategy for navigating the error landscape and finding the best set of weights.\n\n\n\n\n\n\n\n\nComponent\nOptimization Mapping\nAnalogy\n\n\n\n\nAlgorithm\nGradient Descent (or its variants)\nThe Mountain Climber. The climber starts somewhere on the error mountain and takes repeated, calculated steps downhill.\n\n\nLearning Rate\nStep Size (\\(\\alpha\\))\nThe Length of the Stride. Too large, and you might jump right over the minimum; too small, and the journey will take forever.\n\n\nStopping Condition\nConvergence Criteria\nThe Summit Signal. Stop climbing when your steps downhill no longer meaningfully improve your altitude (error).\n\n\n\nOptimization takes the direction provided by Calculus and applies a disciplined strategy (Gradient Descent) to find the weights that minimize the Log-Loss. By iteratively updating the weight vector \\(W\\) using the rule: \\(W_{new} = W_{old} - \\alpha \\cdot \\nabla L\\), we ensure the model continuously improves its ability to correctly separate the classes."
  },
  {
    "objectID": "pages/slt/logistic-regression/index.html#sample-code-logistic-regression-in-python",
    "href": "pages/slt/logistic-regression/index.html#sample-code-logistic-regression-in-python",
    "title": "üéì Logistic Regression Essentials",
    "section": "",
    "text": "Here is a simple example using the popular scikit-learn library to perform a Logistic Regression classification.\n# Import necessary libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n# 1. Prepare Synthetic Data (Feature X, Target Y)\n# X: Study hours, Y: Pass/Fail (1/0)\nX = np.array([[0.5], [1.0], [2.2], [3.1], [4.5], [5.0], [6.8], [7.2], [8.0], [9.5]])\ny = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1]) # 1 = Pass, 0 = Fail\n\n# Reshape X for scikit-learn\nX = X.reshape(-1, 1)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# 2. Initialize and Train the Model\nmodel = LogisticRegression()\n# The .fit() method here is the Optimization process (Gradient Descent)\nmodel.fit(X_train, y_train) \n\n# 3. Make Predictions\n# Predict class labels (0 or 1)\npredictions = model.predict(X_test)\n# Predict probabilities (e.g., 0.95, 0.12)\nprobabilities = model.predict_proba(X_test) \n\n# 4. Evaluate Performance\naccuracy = accuracy_score(y_test, predictions)\n\nprint(f\"Model Intercept (b0): {model.intercept_[0]:.2f}\")\nprint(f\"Model Coefficient (w1): {model.coef_[0][0]:.2f}\")\nprint(f\"Accuracy on Test Data: {accuracy:.2f}\")\nprint(\"\\nProbabilities for Test Data:\")\nprint(probabilities)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aritificial Intelligence and Machine Learning",
    "section": "",
    "text": "This is the home page for the Artificial Intelligence and Machine Learning training program. Here you will find resources, papers, and workshops related to AI and ML. Feel free to explore the site using the navigation bar above."
  }
]