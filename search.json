[
  {
    "objectID": "pages/slt/svm/index.html#what-is-svm-the-widest-street-algorithm",
    "href": "pages/slt/svm/index.html#what-is-svm-the-widest-street-algorithm",
    "title": "üéì Support Vector Machines (SVM) and Linear Algebra",
    "section": "üéØ What is SVM? (The ‚ÄúWidest Street‚Äù Algorithm)",
    "text": "üéØ What is SVM? (The ‚ÄúWidest Street‚Äù Algorithm)\nWhile Logistic Regression focuses on probability, SVM focuses on geometry. Its goal is not just to separate two classes, but to separate them with the widest possible gap.\n\nAnalogy: The Demilitarized Zone (DMZ)\nImagine two warring countries (Class A and Class B). A peace treaty requires a border between them. * Bad Border (Logistic Regression might pick this): A line that runs right next to a Class A village. It‚Äôs technically correct, but risky. * SVM Border: SVM looks for the ‚Äúwidest possible river‚Äù or DMZ that can fit between the closest villages of both countries. The center of this river is the decision boundary. * Support Vectors: These are the specific villages (data points) located right on the edge of the river. They ‚Äúsupport‚Äù or define the boundary. If you move the other villages, the border doesn‚Äôt change. If you move the Support Vectors, the border moves.",
    "crumbs": [
      "Home",
      "SVM & LA"
    ]
  },
  {
    "objectID": "pages/slt/svm/index.html#the-linear-algebra-perspective-data-as-vectors",
    "href": "pages/slt/svm/index.html#the-linear-algebra-perspective-data-as-vectors",
    "title": "üéì Support Vector Machines (SVM) and Linear Algebra",
    "section": "üèóÔ∏è 1. The Linear Algebra Perspective: Data as Vectors",
    "text": "üèóÔ∏è 1. The Linear Algebra Perspective: Data as Vectors\nIn SVM, Linear Algebra is not just a container; it is the logic of the entire algorithm.\n\nA. Data Representation (Vectors and Matrices)\nEvery data point is a vector in space. * The Input (\\(x\\)): A vector pointing from the origin \\((0,0)\\) to the data point‚Äôs location. * The ‚ÄúStreet‚Äù Normal (\\(w\\)): A vector that is perpendicular (at 90 degrees) to the boundary line. It dictates the orientation of the street. * The ‚ÄúStreet‚Äù Position (\\(b\\)): A scalar that dictates the position of the street relative to the origin.\n\n\nB. The Geometric Interpretation (The Hyperplane)\nThe decision boundary is defined by the Linear Algebra equation of a hyperplane:\n\\[w \\cdot x + b = 0\\]\n\n\\(w \\cdot x\\) (The Dot Product): This is the projection of the data point \\(x\\) onto the direction \\(w\\). It measures ‚Äúhow far‚Äù the point is in the direction of the street‚Äôs orientation.\nThe Decision:\n\nIf \\(w \\cdot x + b &gt; 0\\), the point is on the ‚ÄúPositive‚Äù side.\nIf \\(w \\cdot x + b &lt; 0\\), the point is on the ‚ÄúNegative‚Äù side.",
    "crumbs": [
      "Home",
      "SVM & LA"
    ]
  },
  {
    "objectID": "pages/slt/svm/index.html#linear-transformations-the-kernel-trick",
    "href": "pages/slt/svm/index.html#linear-transformations-the-kernel-trick",
    "title": "üéì Support Vector Machines (SVM) and Linear Algebra",
    "section": "üîÆ 2. Linear Transformations: The Kernel Trick",
    "text": "üîÆ 2. Linear Transformations: The Kernel Trick\nThis is where SVM becomes magical using Linear Algebra. Sometimes, data cannot be separated by a straight line (e.g., a red circle inside a blue ring).\n\nThe Concept: Mapping to Higher Dimensions\nWe use a function \\(\\phi(x)\\) to transform our input vector \\(x\\) (2D) into a higher-dimensional vector \\(z\\) (3D or more).\n\nAnalogy: If you have red and blue marbles mixed on a flat table (2D) and can‚Äôt separate them with a stick, you can slap the table (add energy/dimension). The red marbles (lighter) fly higher than the blue ones. Now, you can slide a flat sheet (hyperplane) between the red and blue marbles in mid-air (3D).\n\n\n\nThe Math: Dot Products as Similarity\nSVM relies entirely on the Dot Product (\\(x_i \\cdot x_j\\)). The dot product is a measure of geometric similarity. * If two vectors point in the same direction, their dot product is large. * If they are perpendicular, it is zero.\nThe Kernel Function \\(K(x_i, x_j)\\) allows us to calculate the dot product in that high-dimensional ‚Äúmid-air‚Äù space without actually doing the math to send the points there. This is the ‚ÄúTrick.‚Äù",
    "crumbs": [
      "Home",
      "SVM & LA"
    ]
  },
  {
    "objectID": "pages/slt/svm/index.html#eigenvalues-and-the-gram-matrix",
    "href": "pages/slt/svm/index.html#eigenvalues-and-the-gram-matrix",
    "title": "üéì Support Vector Machines (SVM) and Linear Algebra",
    "section": "üß© 3. Eigenvalues and The Gram Matrix",
    "text": "üß© 3. Eigenvalues and The Gram Matrix\nYou asked how Eigenvalues appear. They are critical to the validity of the Kernel.\nFor a Kernel (like the Gaussian/RBF kernel) to be valid, the matrix of all dot products between all data points (called the Gram Matrix or Kernel Matrix, \\(K\\)) must be Positive Semi-Definite (PSD).\n\nVisualizing the Gram Matrix (\\(K\\))\nImagine a spreadsheet where row \\(i\\) and column \\(j\\) contains the similarity score between data point \\(i\\) and data point \\(j\\).\n\\[K = \\begin{bmatrix}\nx_1 \\cdot x_1 & x_1 \\cdot x_2 & \\dots \\\\\nx_2 \\cdot x_1 & x_2 \\cdot x_2 & \\dots \\\\\n\\vdots & \\vdots & \\ddots\n\\end{bmatrix}\\]\n\n\nThe Role of Eigenvalues (\\(\\lambda\\))\n\nThe Condition: For the math to work (for the optimization to find a unique ‚Äúbest‚Äù solution), all Eigenvalues (\\(\\lambda\\)) of this matrix must be non-negative (\\(\\lambda \\ge 0\\)).\nThe Interpretation: If an eigenvalue is negative, it implies a ‚Äúnegative distance‚Äù or a twisted space where the geometry breaks down. The eigenvalues guarantee that the high-dimensional space we are mapping to ‚Äúexists‚Äù and behaves like normal Euclidean geometry.",
    "crumbs": [
      "Home",
      "SVM & LA"
    ]
  },
  {
    "objectID": "pages/slt/logistic-regression/index.html#what-is-logistic-regression",
    "href": "pages/slt/logistic-regression/index.html#what-is-logistic-regression",
    "title": "üéì Logistic Regression Essentials",
    "section": "üéØ What is Logistic Regression?",
    "text": "üéØ What is Logistic Regression?\nLogistic Regression is one of the most fundamental and widely used algorithms for classification tasks in Machine Learning.\n\nLinear Regression predicts a continuous value (like house price, temperature).\nLogistic Regression predicts a category or probability (like: Will this customer click an ad? Yes/No.¬†Is this email spam? Yes/No).\n\nThe key idea is that instead of predicting the value \\(Y\\) itself, Logistic Regression predicts the probability \\(P(Y=1)\\) that a given input belongs to a certain class. It then uses a special function, called the Sigmoid Function to squash any output into a probability score between 0 and 1. The Sigmoid Function is defined as:\n\\(P(Y=1) = \\frac{1}{1 + e^{-(X \\cdot W)}}\\) ,\nThe Sigmoid function takes the linear combination of input features (\\(X\\)) and weights (\\(W\\)) and transforms it into a probability.\nThe output can then be thresholded (commonly at 0.5) to make a binary classification decision:\n\nIf \\(P(Y=1) \\geq 0.5\\), predict class 1 (YES).\nIf \\(P(Y=1) &lt; 0.5\\), predict class 0 (NO).\n\nThe Sigmoid function has an ‚ÄúS‚Äù-shaped curve that smoothly transitions from 0 to 1, making it ideal for modeling probabilities.\nThe logistic curve is shown below: \n\nAnalogy: The Probability Gatekeeper\nImagine a ‚ÄúProbability Gatekeeper‚Äù with a dial that can only point to values between 0 and 1.\n\nA standard Linear Regression model first calculates a raw score for a data point (this score can be any number, e.g., \\(-10\\), \\(0\\), \\(50\\)). This is the input to the Gatekeeper.\nThe Gatekeeper (the Sigmoid Function) takes this raw score and transforms it:\n\nVery large positive scores get turned into a probability close to \\(\\mathbf{1}\\) (High Confidence: YES).\nVery large negative scores get turned into a probability close to \\(\\mathbf{0}\\) (High Confidence: NO).\nA score of \\(\\mathbf{0}\\) gets turned into a probability of \\(\\mathbf{0.5}\\) (Uncertain).\n\n\nThis transformation allows us to use a linear model for a non-linear classification problem.",
    "crumbs": [
      "Home",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "pages/slt/logistic-regression/index.html#deconstructing-logistic-regression-through-the-four-lenses",
    "href": "pages/slt/logistic-regression/index.html#deconstructing-logistic-regression-through-the-four-lenses",
    "title": "üéì Logistic Regression Essentials",
    "section": "üèóÔ∏è Deconstructing Logistic Regression Through the Four Lenses",
    "text": "üèóÔ∏è Deconstructing Logistic Regression Through the Four Lenses\n\n1. The Linear Algebra Perspective: Data as Mathematical Objects\nCore Idea: Linear Algebra provides the blueprint for how we structure data and calculate the raw score.\n\n\n\n\n\n\n\n\nComponent\nLinear Algebra Mapping\nAnalogy\n\n\n\n\nInput Features\nA Design Matrix (\\(X\\))\nA collection of ingredients for a recipe (e.g., age, income, time spent on site).\n\n\nModel Parameters\nA Weight Vector (\\(W\\))\nA scaling factor for each ingredient (how important is age vs.¬†income?).\n\n\nRaw Score Calculation\nMatrix-Vector Multiplication (\\(X \\cdot W\\))\nThe initial mixing of ingredients to get a base flavor (the raw score).\n\n\n\nJust as in linear regression, the initial step in logistic regression is fundamentally a linear transformation: a weighted sum of the input features. This provides the logit, or log-odds, which is the raw, unbounded score that the Sigmoid function will process.\n\n\n2. The Statistical Foundation: Measuring and Modeling Uncertainty\nCore Idea: Probability & Statistics provides the specific function to model the probability and the metric to measure the model‚Äôs performance.\n\n\n\n\n\n\n\n\nComponent\nStatistical Mapping\nAnalogy\n\n\n\n\nPrediction Function\nSigmoid Function (\\(P = \\frac{1}{1 + e^{-(X \\cdot W)}}\\))\nThe Probability Gatekeeper that converts the raw score into a \\(0\\) to \\(1\\) probability.\n\n\nLoss Function\nLog-Loss (or Binary Cross-Entropy)\nThe Critic‚Äôs Scorecard. Instead of measuring squared distance, it heavily penalizes confident wrong predictions.\n\n\n\nIn classification, we care about assigning the correct probability to the correct class. If the model predicts an event will happen with \\(P=0.9\\) (90% sure) but it doesn‚Äôt happen, the Log-Loss gives a massive penalty. Conversely, if it predicts \\(P=0.1\\) and the event doesn‚Äôt happen, the penalty is small. This function is derived from the principle of Maximum Likelihood Estimation, which seeks the model parameters that make the observed data most probable.\n\\[L(W) = - \\frac{1}{N} \\sum_{i=1}^N \\left[ y_i \\log(\\hat{y}_i) + (1 - y_i) \\log(1 - \\hat{y}_i) \\right]\\]\n\n\n3. The Calculus Engine: The Mathematics of Improvement\nCore Idea: Calculus provides the tool to efficiently determine how to adjust the model parameters to minimize the error.\n\n\n\n\n\n\n\n\nComponent\nCalculus Mapping\nAnalogy\n\n\n\n\nOptimization Direction\nThe Gradient (\\(\\nabla L\\))\nThe Compass Needle. It points in the direction of steepest ascent (worst performance).\n\n\nLearning Mechanism\nPartial Derivatives\nThe Sensitivity Report. For every weight, it tells you: ‚ÄúIf I slightly increase this weight, how much will the total error change?‚Äù\n\n\n\nTo make the model better, we must take steps in the opposite direction of the gradient (\\(\\mathbf{- \\nabla L}\\)). Calculating the gradient of the complex Log-Loss function involves the Chain Rule, where we compute the derivative of the loss with respect to the prediction, then the derivative of the prediction (Sigmoid) with respect to the raw score, and finally the derivative of the raw score with respect to the weights. This systematic calculation is the engine of learning.\n\n\n4. The Optimization Process: The Journey to Better Solutions\nCore Idea: Optimization is the strategy for navigating the error landscape and finding the best set of weights.\n\n\n\n\n\n\n\n\nComponent\nOptimization Mapping\nAnalogy\n\n\n\n\nAlgorithm\nGradient Descent (or its variants)\nThe Mountain Climber. The climber starts somewhere on the error mountain and takes repeated, calculated steps downhill.\n\n\nLearning Rate\nStep Size (\\(\\alpha\\))\nThe Length of the Stride. Too large, and you might jump right over the minimum; too small, and the journey will take forever.\n\n\nStopping Condition\nConvergence Criteria\nThe Summit Signal. Stop climbing when your steps downhill no longer meaningfully improve your altitude (error).\n\n\n\nOptimization takes the direction provided by Calculus and applies a disciplined strategy (Gradient Descent) to find the weights that minimize the Log-Loss. By iteratively updating the weight vector \\(W\\) using the rule: \\(W_{new} = W_{old} - \\alpha \\cdot \\nabla L\\), we ensure the model continuously improves its ability to correctly separate the classes.",
    "crumbs": [
      "Home",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "pages/slt/logistic-regression/index.html#sample-code-logistic-regression-in-python",
    "href": "pages/slt/logistic-regression/index.html#sample-code-logistic-regression-in-python",
    "title": "üéì Logistic Regression Essentials",
    "section": "üíª Sample Code: Logistic Regression in Python",
    "text": "üíª Sample Code: Logistic Regression in Python\nHere is a simple example using the popular scikit-learn library to perform a Logistic Regression classification.\n# Import necessary libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport numpy as np\n\n# 1. Prepare Synthetic Data (Feature X, Target Y)\n# X: Study hours, Y: Pass/Fail (1/0)\nX = np.array([[0.5], [1.0], [2.2], [3.1], [4.5], [5.0], [6.8], [7.2], [8.0], [9.5]])\ny = np.array([0, 0, 0, 0, 1, 1, 1, 1, 1, 1]) # 1 = Pass, 0 = Fail\n\n# Reshape X for scikit-learn\nX = X.reshape(-1, 1)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\n# 2. Initialize and Train the Model\nmodel = LogisticRegression()\n# The .fit() method here is the Optimization process (Gradient Descent)\nmodel.fit(X_train, y_train) \n\n# 3. Make Predictions\n# Predict class labels (0 or 1)\npredictions = model.predict(X_test)\n# Predict probabilities (e.g., 0.95, 0.12)\nprobabilities = model.predict_proba(X_test) \n\n# 4. Evaluate Performance\naccuracy = accuracy_score(y_test, predictions)\n\nprint(f\"Model Intercept (b0): {model.intercept_[0]:.2f}\")\nprint(f\"Model Coefficient (w1): {model.coef_[0][0]:.2f}\")\nprint(f\"Accuracy on Test Data: {accuracy:.2f}\")\nprint(\"\\nProbabilities for Test Data:\")\nprint(probabilities)",
    "crumbs": [
      "Home",
      "Logistic Regression"
    ]
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Aritificial Intelligence and Machine Learning",
    "section": "",
    "text": "This is the home page for the Artificial Intelligence and Machine Learning training program. Here you will find resources, papers, and workshops related to AI and ML. Feel free to explore the site using the navigation bar above."
  },
  {
    "objectID": "pages/slt/mathematics/linear-algebra/analogy-1.html",
    "href": "pages/slt/mathematics/linear-algebra/analogy-1.html",
    "title": "AIML Course",
    "section": "",
    "text": "analogy-1",
    "crumbs": [
      "Home",
      "Analogies LA"
    ]
  },
  {
    "objectID": "pages/slt/mathematics/linear-algebra/analogy-1.html#matrix-multiplication-the-outfit-designer-story",
    "href": "pages/slt/mathematics/linear-algebra/analogy-1.html#matrix-multiplication-the-outfit-designer-story",
    "title": "AIML Course",
    "section": "1. Matrix Multiplication ‚Äì The Outfit Designer Story",
    "text": "1. Matrix Multiplication ‚Äì The Outfit Designer Story\n\nYou‚Äôre helping a school design outfits for different events: sports day, annual day, and farewell. You have shirts, pants, and shoes in different colors.\n\n\n‚ÄúWe can think of the clothes as numbers in a list ‚Äî like a vector ‚Äî and the outfit designer‚Äôs plan as a matrix.‚Äù\n\nLet‚Äôs define:\n\\(\\text{Vector of available clothes:} v = \\begin{bmatrix} 2 \\\\ 3 \\\\ 1 \\end{bmatrix}\\)\n(where 2 shirts, 3 pants, and 1 pair of shoes are available).\nAnd the style matrix for creating different outfits:\n\\(M = \\begin{bmatrix}\n1 & 0 & 1 \\\\   % sports outfit\n0 & 1 & 1 \\\\   % annual day outfit\n1 & 1 & 0      % farewell outfit\n\\end{bmatrix}\\)\n\nRow 1 = Sports outfit uses 1 shirt + 0 pant + 1 shoe.\n\nRow 2 = Annual outfit uses 0 shirt + 1 pant + 1 shoe.\n\nRow 3 = Farewell outfit uses 1 shirt + 1 pant + 0 shoe.\n\nMatrix multiplication:\n\\(M \\times v =\n\\begin{bmatrix}\n1 & 0 & 1 \\\\\n0 & 1 & 1 \\\\\n1 & 1 & 0\n\\end{bmatrix}\n\\begin{bmatrix}\n2 \\\\ 3 \\\\ 1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n(1√ó2 + 0√ó3 + 1√ó1) \\\\\n(0√ó2 + 1√ó3 + 1√ó1) \\\\\n(1√ó2 + 1√ó3 + 0√ó1)\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n3 \\\\ 4 \\\\ 5\n\\end{bmatrix}\\)\n\n‚ÄúThis means we can make 3 sports outfits, 4 annual day outfits, and 5 farewell outfits. The matrix acted like a recipe book that transformed our list of clothes into finished outfits.‚Äù",
    "crumbs": [
      "Home",
      "Analogies LA"
    ]
  },
  {
    "objectID": "pages/slt/mathematics/linear-algebra/analogy-1.html#eigenvectors-and-eigenvalues-the-stretchy-line-story",
    "href": "pages/slt/mathematics/linear-algebra/analogy-1.html#eigenvectors-and-eigenvalues-the-stretchy-line-story",
    "title": "AIML Course",
    "section": "2. Eigenvectors and Eigenvalues ‚Äì The Stretchy Line Story",
    "text": "2. Eigenvectors and Eigenvalues ‚Äì The Stretchy Line Story\n\nImagine you draw lines on a rubber sheet ‚Äî one horizontal, one diagonal. Now you stretch the sheet by pulling it rightward.\n\n\n‚ÄúSome lines will twist and tilt, but one or two special lines won‚Äôt turn direction; they only get longer or shorter.‚Äù\n\nWe can represent the transformation as:\n\\(A = \\begin{bmatrix} 2 & 0 \\\\ 0 & 1 \\end{bmatrix}\\)\nThis means x-values are doubled, but y-values stay the same.\nFor a vector along the x-axis:\n\\(v_1 = \\begin{bmatrix} 1 \\\\ 0 \\end{bmatrix}\\)\nthen\n\\(A v_1 = \\begin{bmatrix} 2 \\\\ 0 \\end{bmatrix} = 2 v_1\\)\nThus, \\(v_1\\) doesn‚Äôt change direction ‚Äî only its length doubles. Its eigenvalue is \\(2\\).\nFor a vector along the y-axis:\n\\(v_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix}\\) then\n\\(A.v_2 = \\begin{bmatrix} 0 \\\\ 1 \\end{bmatrix} = 1 v_2\\)\nHere, the direction stays the same, and the eigenvalue is \\(1\\).\nTeacher explains:\n&gt; ‚ÄúThe eigenvectors are the lines that don‚Äôt rotate when stretched, only change in size. The eigenvalues tell how much each line grows or shrinks.‚Äù",
    "crumbs": [
      "Home",
      "Analogies LA"
    ]
  },
  {
    "objectID": "pages/slt/mathematics/linear-algebra/analogy-1.html#dimensionality-reduction-the-shadow-story",
    "href": "pages/slt/mathematics/linear-algebra/analogy-1.html#dimensionality-reduction-the-shadow-story",
    "title": "AIML Course",
    "section": "3. Dimensionality Reduction ‚Äì The Shadow Story",
    "text": "3. Dimensionality Reduction ‚Äì The Shadow Story\n\nYou have a 3D toy (a cube) lit by a lamp. Its shadow falls on the wall ‚Äî a 2D picture that keeps most of the cube‚Äôs shape.\n\n\n‚ÄúWhen we look at the shadow, we‚Äôre reducing a 3D object to 2D, keeping its most important features.‚Äù\n\nLet‚Äôs imagine three points on the cube in 3D space:\n\\(P_1 = (1, 2, 3), \\quad P_2 = (2, 3, 4), \\quad P_3 = (3, 5, 6)\\)\nTo project onto a 2D plane (ignore the z-axis):\n\\(\\text{Projection matrix }\nP = \\begin{bmatrix}\n1 & 0 & 0 \\\\\n0 & 1 & 0\n\\end{bmatrix}\\)\nThen:\n\\(P \\times\n\\begin{bmatrix}\n1 \\\\ 2 \\\\ 3\n\\end{bmatrix} =\n\\begin{bmatrix}\n1 \\\\ 2\n\\end{bmatrix},\n\\quad\nP \\times\n\\begin{bmatrix}\n2 \\\\ 3 \\\\ 4\n\\end{bmatrix}=\n\\begin{bmatrix}\n2 \\\\ 3\n\\end{bmatrix},\n\\quad\nP \\times\n\\begin{bmatrix}\n3 \\\\ 5 \\\\ 6\n\\end{bmatrix}=\n\\begin{bmatrix}\n3 \\\\ 5\n\\end{bmatrix}\\)\n\n‚ÄúThe shadow keeps the main structure ‚Äî how points relate ‚Äî but loses some details (depth). That‚Äôs exactly what dimensionality reduction like PCA does: it keeps the most useful parts and drops unnecessary ones.‚Äù",
    "crumbs": [
      "Home",
      "Analogies LA"
    ]
  }
]